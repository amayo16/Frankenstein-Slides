library(reshape2)
install.packages("reshape2")
library(reshape2)
acast(word~sentiment,value.var = 'freq',fill = 0)
acast(dracula_total,word~sentiment,value.var = 'freq',fill = 0)
dracula_matrix <- acast(dracula_total,word~sentiment,value.var = 'freq',fill = 0)
dracula_matrix
head(dracula_matrix)
comparison.cloud(dracula_matrix)
?comparison.cloud
comparison.cloud(dracula_matrix, colors = c("orange","black"))
comparison.cloud(dracula_matrix, colors = c("orange","black"))
## Packages for Social Network Analysis
library(igraph)
library(graphTweets)
### Scraping Twitter Data in R and Text Analysis
library(twitteR)
library(ROAuth)
library(httr)
library(plyr)
library(tm)
library(topicmodels)
### Sentiment Analysis with Twitter Data
library(syuzhet)
library(ggplot2)
library(scales)
library(reshape2)
library(dplyr)
# Set API Keys
api_key <- "LtiT6rsdGHjJGJa3104vPvRFV"
api_secret <- "mgCehFgalvYNOFPm8JWDHtdRyLD3kEi7zxjiXhFRr6NSFN6xzI"
access_token <- "2845584590-AXBUOeWb89Qpfqx6EK6KISlUZAiLmHjwlwrPLMt"
access_token_secret <- "tos2pqB1v1CPAm8IzMsdmb86YgUX7ZwTeEVQCqj5D9KST"
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
tweets <- searchTwitter("sinai", n=3000, lang="en")
total_en <- read.csv(choose.files())
total_en
head(total_en
)
## Packages for Social Network Analysis
library(igraph)
library(graphTweets)
### Scraping Twitter Data in R and Text Analysis
library(twitteR)
library(ROAuth)
library(httr)
library(plyr)
library(tm)
library(topicmodels)
### Sentiment Analysis with Twitter Data
library(syuzhet)
library(ggplot2)
library(scales)
library(reshape2)
library(dplyr)
# Set API Keys
api_key <- "LtiT6rsdGHjJGJa3104vPvRFV"
api_secret <- "mgCehFgalvYNOFPm8JWDHtdRyLD3kEi7zxjiXhFRr6NSFN6xzI"
access_token <- "2845584590-AXBUOeWb89Qpfqx6EK6KISlUZAiLmHjwlwrPLMt"
access_token_secret <- "tos2pqB1v1CPAm8IzMsdmb86YgUX7ZwTeEVQCqj5D9KST"
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
tweets <- searchTwitter("sinai", n=3000, lang="en")
## Packages for Social Network Analysis
library(igraph)
library(graphTweets)
### Scraping Twitter Data in R and Text Analysis
library(twitteR)
library(ROAuth)
library(httr)
library(plyr)
library(tm)
library(topicmodels)
### Sentiment Analysis with Twitter Data
library(syuzhet)
library(ggplot2)
library(scales)
library(reshape2)
library(dplyr)
# Set API Keys
api_key <- "LtiT6rsdGHjJGJa3104vPvRFV"
api_secret <- "mgCehFgalvYNOFPm8JWDHtdRyLD3kEi7zxjiXhFRr6NSFN6xzI"
access_token <- "2845584590-AXBUOeWb89Qpfqx6EK6KISlUZAiLmHjwlwrPLMt"
access_token_secret <- "tos2pqB1v1CPAm8IzMsdmb86YgUX7ZwTeEVQCqj5D9KST"
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
tweets <- searchTwitter("sinai", n=3000, lang="en")
tw_df <- twListToDF(tweets)
# Labeling our network
classified = cbind(tw_df, rep("sinai", nrow(tw_df)))
colnames(classified)[ncol(classified)]="classifier"
tw_df = classified
tweets2 <- searchTwitter("coptic", n=3000)
tw_df_2 <- twListToDF(tweets2)
# Labeling our network
classified = cbind(tw_df_2, rep("coptic", nrow(tw_df_2)))
colnames(classified)[ncol(classified)]="classifier"
tw_df_2 = classified
tweets3 <- searchTwitter("Sisi", n=3000)
tw_df_2
View(tw_df_2
)
total_en = rbind.fill(tw_df_2, tw_df)
str(total_en)
#### Text Preprocessing
sk = total_en$text
TextPreprocessing = lapply(sk, function(x) {
x = gsub('http\\S+\\s*', '', x) ## Remove URLs
x = gsub('\\b+RT', '', x) ## Remove RT
x = gsub('#\\S+', '', x) ## Remove Hashtags
x = gsub('@\\S+', '', x) ## Remove Mentions
x = gsub('[[:cntrl:]]', '', x) ## Remove Controls and special characters
x = gsub("\\d", '', x) ## Remove Controls and special characters
x = gsub('[[:punct:]]', '', x) ## Remove Punctuations
x = gsub("^[[:space:]]*","",x) ## Remove leading whitespaces
x = gsub("[[:space:]]*$","",x) ## Remove trailing whitespaces
})
# or as a vector
bd_list = as.vector(TextPreprocessing)
mycorpus <- Corpus(VectorSource(bd_list))
mycorpus = tm_map(mycorpus, content_transformer(function(x) iconv(x, to='UTF-8', sub='byte')))
### Transformer all characters to lower case
mycorpus = tm_map(mycorpus, content_transformer(tolower))
### Remove all Punctuation
mycorpus = tm_map(mycorpus, removePunctuation)
### Remove all Numbers
mycorpus = tm_map(mycorpus, removeNumbers)
### Remove Stopwords
mycorpus = tm_map(mycorpus, removeWords, stopwords('english'))
#### transform to Document Term Matrix
skip.dtm = DocumentTermMatrix(mycorpus)
### Topic Model Analysis
rowTotals = apply(skip.dtm, 1, sum)
smtpmodel = skip.dtm[rowTotals>0, ]
smmodel_tweets = LDA(smtpmodel, 5)
terms(smmodel_tweets , 40)
### Preparation for Tableau
## Create dataframe of discovered topics
topic_col=topics(smmodel_tweets)
topic_col2=as.data.frame(topic_col)
### ONLY IF NEEDED (if rows are missing) Empty rows identifier for Topic Model JSON Application
empty.rows <- skip.dtm[rowTotals == 0, ]$dimnames[1][[1]]
## Find out the missing rows
empty.rows
## Remove empty row identified as "951" from tw_df dataframe
## For your analysis you may have more than one missing row.
## If you have more than one missing row, the code should look
## as follows
## For example   new.dtm.df = tw_df[-c(951, 224, 301, 501), ]
##Remember if you are not missing any rows, or rather your rows match
## for topic_col2   and tw_df   then you do not need to do this operation
## Just combine the two dataframes using line 14
ix = which(rownames(total_en) %in% c(empty.rows))
clean = total_en[-ix, ]
date_tab = clean$created
### Adding Topic Models to the Total dataframe
new_tw_df_7 = cbind(date_tab, topic_col2)
new_tw_df_8 = cbind(clean, topic_col2)
str(new_tw_df_8)
## Prepare for sentiment analysis
library(syuzhet)
clean$text <- iconv(clean$text, 'UTF-8', 'ASCII')
# Get Sentiment Analysis
comb_sent = get_nrc_sentiment(clean$text)
#combining Dataframes
total_en_final = cbind(new_tw_df_8, comb_sent)
edges <- getEdges(data = total_en_final, tweets = "text", source = "screenName", "retweetCount", "anger", "trust", "positive", "negative", "classifier", "topic_col", str.length = 20)
nodes <- getNodes(edges, source = "source", target="target")
g <- graph.data.frame(edges, directed = TRUE, vertices = nodes)
write.graph(g, file="CoptvsSinai_Nov15_2.graphml", format="graphml")
getwd()
write.csv(total_en_final, "CoptvsSinai_Nov15_2.csv", row.names = FALSE)
train <- sample(c(TRUE ,FALSE), nrow(Hitters),rep = TRUE)
test <- (!train)
library(glmnet)
library(ISLR)
library(caret)
library(leaps)
Hitters <- na.omit(Hitters)
train <- sample(c(TRUE ,FALSE), nrow(Hitters),rep = TRUE)
test <- (!train)
test
regfit.ss = regsubsets(Salary∼., data = Hitters[train,], nvmax =19)
test.mat <- model.matrix(Salary∼., data = Hitters[test,])
test.mat
test.mat
regfit.ss
val.errors
# get MSE using test data
val.errors =rep(NA ,19)
for(i in 1:19)
{
coefi <- coef(regfit.ss, id=i)
pred <- test.mat[,names(coefi)]%*% coefi # matrix multiplication
val.errors[i] <- mean(( Hitters$Salary[test]-pred)^2)
}
val.errors
predict.regsubsets = function(object, newdata, id,...)
{
form = as.formula(object$call[[2]]) # extract model ("call")
mat = model.matrix (form, newdata)
coefi = coef(object, id=id)
xvars = names(coefi)
mat[,xvars] %*% coefi
}
regfit.ss = regsubsets(Salary∼., data=Hitters, nvmax =19)
k = 10
set.seed(7)
folds = sample(1:k, nrow(Hitters), replace = TRUE)
# set up cv MSE table
cv.mse = matrix (NA ,k,19, dimnames =list(NULL , paste (1:19) ))
# loop through all folds
for(j in 1:k)
{
best.ss = regsubsets (Salary ~., data = Hitters [folds != j,], nvmax =19)
# compile mean MSE of all folds for each number of predictors
for(i in 1:19)
{
pred = predict(best.ss, Hitters[folds == j,], id=i)
cv.mse[j,i] = mean((Hitters$Salary[folds == j] - pred)^2)
}
}
cv.mse
x <- model.matrix (Salary∼.,Hitters )[,-1]
y <- Hitters$Salary
require(glmnet)
# set up a grid of values for lambda; the higher the lambda, the greater the shrinkage penalty
grid <- 10^seq (10,-2, length =100)
ridge.mod <- glmnet(x, y, alpha = 0, lambda = grid)
coef(ridge.mod) # 20 predictors, 100 lambdas
# let's get the mean MSE in a test sample
set.seed(7)
train <- sample(1:nrow(x), nrow(x)/2)
test <- (-train )
y.test <- y[test]
# use the grid again
ridge.mod <- glmnet(x[train,], y[train], alpha =0, lambda =grid, thresh = 1e-12)
# predict using glmnet : note the use of newx
ridge.pred <- predict(ridge.mod, s = 4, newx = x[test ,])
mean((ridge.pred - y.test)^2)
hitter.mat <- model.matrix(Salary∼., data = Hitters[test,])
HitPCV <- prcomp(hitter.mat[,2:20])
HitPCV
movies <- read.csv(choose.files())
movies
head(movies)
colnames(movies) <- c("Film","Genre","CriticRating","AudienceRating","BudgetMillions","Year")
head(movies)
str(movies)
summary(movies)
factor(movies$Year)
movies$Year <- factor(movies$Year)
summary(movies)
library(ggplot2)
ggplot(data = movies, aes(x = CriticRating, y = AudienceRating))
ggplot(data = movies, aes(x = CriticRating, y = AudienceRating))+
geom_point()
ggplot(data = movies, aes(x = CriticRating, y = AudienceRating, color = Genre))+
geom_point()
ggplot(data = movies, aes(x = CriticRating, y = AudienceRating,
color = Genre, size = Genre))+
geom_point()
ggplot(data = movies, aes(x = CriticRating, y = AudienceRating,
color = Genre, size = Budget))+
geom_point()
ggplot(data = movies, aes(x = CriticRating, y = AudienceRating,
color = Genre, size = BudgetMillions))+
geom_point()
p <- ggplot(data = movies, aes(x = CriticRating, y = AudienceRating,
color = Genre, size = BudgetMillions))
p + geom_point()
p+ geom_line()
p + geom_point() + geom_line()
p + geom_line() + geom_point()
q <- ggplot(data = movies, aes(x = CriticRating, y = AudienceRating,
color = Genre, size = BudgetMillions))
q + geom_point()
q + geom_point(aes(size = CriticRating))
q + geom_point(aes(color = BudgetMillions))
q + geom_point(aes(x = BudgetMillions))
q + geom_point(aes(x = BudgetMillions)) +
xlab("Budget in Millions")
p + geom_line(size = 1) + geom_point()
r <- ggplot(data = movies, aes(x=CriticRating, y = AudienceRating))
r + geom_point()
r + geom_point(aes(color = Genre))
r + geom_point(color = "DarkGreen")
r + geom_point(aes(size = BudgetMillions))
r + geom_point(size = 10)
r + geom_point(size = BudgetMillions)
r + geom_point(size = 5)
s <- ggplot(data = movies, aes(x = BudgetMillions))
s + geom_histogram(binwidth = 10)
s + geom_histogram(binwidth = 10, aes(color = Genre))
s + geom_histogram(binwidth = 10, aes(fill = Genre))
s + geom_histogram(binwidth = 10, color = "Black" aes(fill = Genre))
s + geom_histogram(binwidth = 10, color = "Black", aes(fill = Genre))
s + geom_density()
s + geom_density(aes(fill = Genre))
s + geom_density(aes(fill = Genre), position = "stack")
library(glmnet)
library(ISLR)
library(caret)
library(leaps)
FStat <- read.csv('http://math.mercyhurst.edu/~sousley/STAT_139/data/FStat.csv')
WMStat <- FStat[which(FStat$PopSex == "WM"),]
WMStat <- WMStat[,c('FSTAT','claxln','scapht','scapbr','humxln','radxln','ulnxln','sacaht','femxln','fembln','tibxln','fibxln')]
head(WMStat)
nrow(WMStat)
WMStat.na <- na.omit(WMStat)
library(pls)
library(ISLR)
pcr.fit <- pcr(FSTAT~., data=WMStat.na, scale = TRUE, validation = "CV")
summary(pcr.fit)
validationplot(pcr.fit ,val.type="MSEP")
train <- sample(c(TRUE, FALSE), nrow(WMStat.na), rep=TRUE)
test <- (!train)
pcr.fit <- pcr(FSTAT~., data=WMStat.na, subset = train, scale = TRUE, validation ="CV")
summary(pcr.fit)
pcr.pred <- predict(pcr.fit, WMStat.na[test,], ncomp = 1)
mean((pcr.pred - WMStat.na[test,"FSTAT"])^2)
pcr.fitt <- pcr(FSTAT~., data=WMStat.na, scale = TRUE, ncomp = 1)
summary(pcr.fitt)
pls.fit <- plsr(FSTAT ~ ., data=WMStat.na, subset = train, scale=T, validation ="CV")
summary(pls.fit)
pls.pred <- predict(pls.fit, WMStat.na[test,], ncomp = 1)
mean((pls.pred - WMStat.na[test,"FSTAT"])^2)
reg.WMS <- regsubsets(FSTAT~., WMStat.na)
summary(reg.WMS)
reg.WMS <- regsubsets(FSTAT ~., WMStat.na, nvmax = 11)
res.reg.WMS <- summary(reg.WMS)
res.reg.WMS$rsq
plot(res.reg.WMS$rss ,xlab=" Number of Variables ",ylab=" RSS", type = 'l')
plot(res.reg.WMS$adjr2 ,xlab =" Number of Variables ", ylab=" Adjusted RSq",type = 'l')
bestset <- which.max(res.reg.WMS$adjr2)
bestset
points(bestset, res.reg.WMS$adjr2[bestset], col = "red",cex = 1.5, pch = 20)
plot(res.reg.WMS$adjr2 ,xlab =" Number of Variables ", ylab=" Adjusted RSq",type = 'l')
points(bestset, res.reg.WMS$adjr2[bestset], col = "red",cex = 1.5, pch = 20)
plot(res.reg.WMS$cp ,xlab =" Number of Variables ", ylab = "Cp", type = 'l')
cpmin <- which.min (res.reg.WMS$cp )
cpmin
points (cpmin, res.reg.WMS$cp [cpmin], col ="red",cex = 1.5, pch = 20)
bicmin <- which.min(res.reg.WMS$bic)
bicmin
plot(res.reg.WMS$bic ,xlab=" Number of Variables ", ylab = " BIC",type = 'l')
points (bicmin, res.reg.WMS$bic [6], col =" red", cex = 2, pch = 20)
coef(reg.WMS, bicmin)
reg.fwd <- regsubsets(FSTAT~., data= WMStat.na ,nvmax = 11, method = "forward")
res.reg.fwd <- summary(reg.fwd)
bicmin2 <- which.min(res.reg.fwd$bic)
bicmin2
coef(reg.WMS, 3)
coef(reg.fwd, 3)
set.seed (7)
train <- sample(c(TRUE ,FALSE), nrow(WMStat.na),rep = TRUE)
test <- (!train)
regfit.ss = regsubsets(FSTAT~., data = WMStat.na[train,], nvmax =11)
test.mat <- model.matrix(FSTAT~., data = WMStat.na[test,])
test.mat
val.errors =rep(NA ,11)
for(i in 1:11)
{
coefi <- coef(regfit.ss, id=i)
pred <- test.mat[,names(coefi)]%*% coefi
val.errors[i] <- mean(( WMStat.na$FSTAT[test]-pred)^2)
}
val.errors
which.min(val.errors)
coef(regfit.ss, 1)
k = 10
set.seed(7)
folds = sample(1:k, nrow(WMStat.na), replace = TRUE)
cv.mse = matrix(NA ,k,11, dimnames =list(NULL , paste (1:11) ))
for(j in 1:k)
{
best.ss = regsubsets (FSTAT ~., data = WMStat.na [folds != j,], nvmax =11)
# compile mean MSE of all folds for each number of predictors
for(i in 1:19)
{
pred = predict(best.ss, WMStat.na[folds == j,], id=i)
cv.mse[j,i] = mean((WMStat.na$FSTAT[folds == j] - pred)^2)
}
}
for(j in 1:k)
{
best.ss = regsubsets (FSTAT ~., data = WMStat.na [folds != j,], nvmax =11)
for(i in 1:11)
{
pred = predict(best.ss, WMStat.na[folds == j,], id=i)
cv.mse[j,i] = mean((WMStat.na$FSTAT[folds == j] - pred)^2)
}
}
best.ss
k = 10
set.seed(7)
folds = sample(1:k, nrow(WMStat.na), replace = TRUE)
cv.mse = matrix(NA ,k,11, dimnames =list(NULL , paste (1:11) ))
for(j in 1:k)
{
best.ss = regsubsets(FSTAT ~., data = WMStat.na [folds != j,], nvmax =11)
for(i in 1:11)
{
pred = predict(best.ss, WMStat.na[folds == j,], id=i)
cv.mse[j,i] = mean((WMStat.na$FSTAT[folds == j] - pred)^2)
}
}
k = 10
set.seed(7)
folds = sample(1:k, nrow(Hitters), replace = TRUE)
# set up cv MSE table
cv.mse = matrix (NA ,k,19, dimnames =list(NULL , paste (1:19) ))
# loop through all folds
for(j in 1:k)
{
best.ss = regsubsets (Salary ~., data = Hitters [folds != j,], nvmax =19)
# compile mean MSE of all folds for each number of predictors
for(i in 1:19)
{
pred = predict(best.ss, Hitters[folds == j,], id=i)
cv.mse[j,i] = mean((Hitters$Salary[folds == j] - pred)^2)
}
}
library(glmnet)
library(ISLR)
library(caret)
library(leaps)
FStat <- read.csv('http://math.mercyhurst.edu/~sousley/STAT_139/data/FStat.csv')
WMStat <- FStat[which(FStat$PopSex == "WM"),]
WMStat <- WMStat[,c('FSTAT','claxln','scapht','scapbr','humxln','radxln','ulnxln','sacaht','femxln','fembln','tibxln','fibxln')]
head(WMStat)
nrow(WMStat)
WMStat.na <- na.omit(WMStat)
#----------------------------Subset Selection
reg.WMS <- regsubsets(FSTAT~., WMStat.na)
summary(reg.WMS)
reg.WMS <- regsubsets(FSTAT ~., WMStat.na, nvmax = 11)
res.reg.WMS <- summary(reg.WMS)
res.reg.WMS$bic
plot(res.reg.WMS$rss ,xlab=" Number of Variables ",ylab=" RSS", type = 'l')
plot(res.reg.WMS$adjr2 ,xlab =" Number of Variables ", ylab=" Adjusted RSq",type = 'l')
bestset <- which.max(res.reg.WMS$adjr2)
bestset
points(bestset, res.reg.WMS$adjr2[bestset], col = "red",cex = 1.5, pch = 20)
plot(res.reg.WMS$cp ,xlab =" Number of Variables ", ylab = "Cp", type = 'l')
cpmin <- which.min (res.reg.WMS$cp )
cpmin
points (cpmin, res.reg.WMS$cp [cpmin], col ="red",cex = 1.5, pch = 20)
bicmin <- which.min(res.reg.WMS$bic)
bicmin
plot(res.reg.WMS$bic ,xlab=" Number of Variables ", ylab = " BIC",type = 'l')
points (bicmin, res.reg.WMS$bic [6], col =" red", cex = 2, pch = 20)
bicmin
coef(reg.WMS, bicmin)
reg.fwd <- regsubsets(FSTAT~., data= WMStat.na ,nvmax = 11, method = "forward")
res.reg.fwd <- summary(reg.fwd)
bicmin2 <- which.min(res.reg.fwd$bic)
bicmin2
coef(reg.WMS, 3)
coef(reg.fwd, 3)
set.seed (7)
train <- sample(c(TRUE ,FALSE), nrow(WMStat.na),rep = TRUE)
test <- (!train)
regfit.ss = regsubsets(FSTAT~., data = WMStat.na[train,], nvmax =11)
test.mat <- model.matrix(FSTAT~., data = WMStat.na[test,])
test.mat
val.errors =rep(NA ,11)
for(i in 1:11)
{
coefi <- coef(regfit.ss, id=i)
pred <- test.mat[,names(coefi)]%*% coefi
val.errors[i] <- mean(( WMStat.na$FSTAT[test]-pred)^2)
}
val.errors
which.min(val.errors)
coef(regfit.ss, 1)
predict.regsubsets = function(object, newdata, id,...)
{
form = as.formula(object$call[[2]]) # extract model ("call")
mat = model.matrix (form, newdata)
coefi = coef(object, id=id)
xvars = names(coefi)
mat[,xvars] %*% coefi
}
k = 10
set.seed(7)
folds = sample(1:k, nrow(WMStat.na), replace = TRUE)
cv.mse = matrix(NA ,k,11, dimnames =list(NULL , paste (1:11) ))
for(j in 1:k)
{
best.ss = regsubsets(FSTAT ~., data = WMStat.na [folds != j,], nvmax =11)
for(i in 1:11)
{
pred = predict(best.ss, WMStat.na[folds == j,], id=i)
cv.mse[j,i] = mean((WMStat.na$FSTAT[folds == j] - pred)^2)
}
}
cv.mse
cv.mse
mean.cv.mse = apply(cv.errors, 3, mean)
mean.cv.mse = apply(cv.mse, 3, mean)
?apply
mean.cv.mse = apply(cv.mse, 7, mean)
cs.mse
cv.mse
mean.cv.mse = apply(cv.mse, 7, mean)
cv.mse[7]
cv.mse[7,]
mean.cv.mse = apply(cv.mse, c(7,), mean)
mean.cv.mse = apply(cv.mse, 7, mean)
mean.cv.mse = apply(cv.mse, 2, mean)
mean.cv.mse
mean.cv.mse
setwd("C:/Users/Andrew/Desktop/Dracula Slides")
